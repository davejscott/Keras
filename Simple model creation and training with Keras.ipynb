{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "# model.add(l4) Can also add layers like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note, Dense is suggesting each layer, this is a 3 layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for i in range(1000):\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davescott/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davescott/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 11s - loss: 0.6779 - acc: 0.5533\n",
      "Epoch 2/20\n",
      " - 11s - loss: 0.6450 - acc: 0.6248\n",
      "Epoch 3/20\n",
      " - 11s - loss: 0.6067 - acc: 0.7000\n",
      "Epoch 4/20\n",
      " - 11s - loss: 0.5670 - acc: 0.7662\n",
      "Epoch 5/20\n",
      " - 11s - loss: 0.5289 - acc: 0.7948\n",
      "Epoch 6/20\n",
      " - 11s - loss: 0.4915 - acc: 0.8319\n",
      "Epoch 7/20\n",
      " - 11s - loss: 0.4551 - acc: 0.8652\n",
      "Epoch 8/20\n",
      " - 11s - loss: 0.4199 - acc: 0.8852\n",
      "Epoch 9/20\n",
      " - 11s - loss: 0.3872 - acc: 0.9010\n",
      "Epoch 10/20\n",
      " - 11s - loss: 0.3558 - acc: 0.9152\n",
      "Epoch 11/20\n",
      " - 11s - loss: 0.3295 - acc: 0.9281\n",
      "Epoch 12/20\n",
      " - 11s - loss: 0.3073 - acc: 0.9224\n",
      "Epoch 13/20\n",
      " - 11s - loss: 0.2891 - acc: 0.9167\n",
      "Epoch 14/20\n",
      " - 11s - loss: 0.2743 - acc: 0.9186\n",
      "Epoch 15/20\n",
      " - 10s - loss: 0.2620 - acc: 0.9190\n",
      "Epoch 16/20\n",
      " - 10s - loss: 0.2521 - acc: 0.9267\n",
      "Epoch 17/20\n",
      " - 10s - loss: 0.2439 - acc: 0.9286\n",
      "Epoch 18/20\n",
      " - 10s - loss: 0.2369 - acc: 0.9319\n",
      "Epoch 19/20\n",
      " - 10s - loss: 0.2312 - acc: 0.9267\n",
      "Epoch 20/20\n",
      " - 10s - loss: 0.2263 - acc: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c23b6d978>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, batch_size=10, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Epochs gives an accuracy of over 93% fit for the model. #notbad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1890 samples, validate on 210 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davescott/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.3134 - acc: 0.9402 - val_loss: 0.0767 - val_acc: 0.9857\n",
      "Epoch 2/20\n",
      " - 10s - loss: 0.3024 - acc: 0.9423 - val_loss: 0.0792 - val_acc: 0.9857\n",
      "Epoch 3/20\n",
      " - 10s - loss: 0.2935 - acc: 0.9418 - val_loss: 0.0842 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      " - 10s - loss: 0.2862 - acc: 0.9418 - val_loss: 0.0849 - val_acc: 0.9857\n",
      "Epoch 5/20\n",
      " - 11s - loss: 0.2799 - acc: 0.9418 - val_loss: 0.0867 - val_acc: 0.9857\n",
      "Epoch 6/20\n",
      " - 10s - loss: 0.2744 - acc: 0.9397 - val_loss: 0.0894 - val_acc: 0.9857\n",
      "Epoch 7/20\n",
      " - 10s - loss: 0.2703 - acc: 0.9397 - val_loss: 0.0917 - val_acc: 0.9857\n",
      "Epoch 8/20\n",
      " - 10s - loss: 0.2669 - acc: 0.9397 - val_loss: 0.0949 - val_acc: 0.9857\n",
      "Epoch 9/20\n",
      " - 10s - loss: 0.2644 - acc: 0.9397 - val_loss: 0.0967 - val_acc: 0.9857\n",
      "Epoch 10/20\n",
      " - 10s - loss: 0.2623 - acc: 0.9397 - val_loss: 0.0993 - val_acc: 0.9857\n",
      "Epoch 11/20\n",
      " - 10s - loss: 0.2609 - acc: 0.9397 - val_loss: 0.1016 - val_acc: 0.9857\n",
      "Epoch 12/20\n",
      " - 10s - loss: 0.2596 - acc: 0.9397 - val_loss: 0.1032 - val_acc: 0.9857\n",
      "Epoch 13/20\n",
      " - 10s - loss: 0.2589 - acc: 0.9397 - val_loss: 0.1042 - val_acc: 0.9857\n",
      "Epoch 14/20\n",
      " - 10s - loss: 0.2581 - acc: 0.9397 - val_loss: 0.1058 - val_acc: 0.9857\n",
      "Epoch 15/20\n",
      " - 10s - loss: 0.2575 - acc: 0.9397 - val_loss: 0.1069 - val_acc: 0.9857\n",
      "Epoch 16/20\n",
      " - 9s - loss: 0.2571 - acc: 0.9397 - val_loss: 0.1077 - val_acc: 0.9857\n",
      "Epoch 17/20\n",
      " - 9s - loss: 0.2567 - acc: 0.9397 - val_loss: 0.1082 - val_acc: 0.9857\n",
      "Epoch 18/20\n",
      " - 9s - loss: 0.2561 - acc: 0.9397 - val_loss: 0.1085 - val_acc: 0.9857\n",
      "Epoch 19/20\n",
      " - 9s - loss: 0.2558 - acc: 0.9397 - val_loss: 0.1096 - val_acc: 0.9857\n",
      "Epoch 20/20\n",
      " - 9s - loss: 0.2555 - acc: 0.9397 - val_loss: 0.1104 - val_acc: 0.9857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c24554b70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, validation_split=0.1, batch_size=10, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model is generalizing well on the validation data. Not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random_younger = randint(13, 64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "for i in range(200):\n",
    "    random_younger = randint(13, 64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davescott/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_test_samples = scaler.fit_transform((test_samples).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9256975  0.07430244]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.98067516 0.01932482]\n",
      "[0.06452356 0.9354764 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.9750354  0.02496454]\n",
      "[0.14107172 0.85892826]\n",
      "[0.9661509  0.03384911]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.03393341 0.96606654]\n",
      "[0.9802137  0.01978632]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.9750354  0.02496454]\n",
      "[0.04911499 0.95088506]\n",
      "[0.86460304 0.13539691]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.9804458  0.01955424]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.973278   0.02672207]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.9819854  0.01801459]\n",
      "[0.14107172 0.85892826]\n",
      "[0.9799788  0.02002111]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.6408268  0.35917315]\n",
      "[0.01757339 0.98242664]\n",
      "[0.9817902  0.01820983]\n",
      "[0.14107172 0.85892826]\n",
      "[0.9817902  0.01820983]\n",
      "[0.37353387 0.6264661 ]\n",
      "[0.9811261  0.01887386]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.93572605 0.06427391]\n",
      "[0.12978986 0.8702101 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.37353387 0.6264661 ]\n",
      "[0.83990735 0.16009264]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.6408268  0.35917315]\n",
      "[0.01321834 0.98678166]\n",
      "[0.9820242  0.01797579]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.9802137  0.01978632]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.1531616  0.84683836]\n",
      "[0.9821016  0.01789844]\n",
      "[0.02123091 0.97876906]\n",
      "[0.9821786  0.01782142]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.90016395 0.099836  ]\n",
      "[0.05894332 0.9410566 ]\n",
      "[0.9821786  0.01782142]\n",
      "[0.04480353 0.9551965 ]\n",
      "[0.88425016 0.11574983]\n",
      "[0.02123091 0.97876906]\n",
      "[0.981672   0.01832796]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.9141024  0.08589759]\n",
      "[0.27777967 0.72222036]\n",
      "[0.97135735 0.02864261]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9804458  0.01955424]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.96287286 0.03712709]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.981672   0.01832796]\n",
      "[0.07718495 0.922815  ]\n",
      "[0.9141024  0.08589759]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9816325  0.01836751]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.9821016  0.01789844]\n",
      "[0.03723959 0.96276045]\n",
      "[0.6895815  0.31041852]\n",
      "[0.14107172 0.85892826]\n",
      "[0.96287286 0.03712709]\n",
      "[0.04911499 0.95088506]\n",
      "[0.9817115 0.0182885]\n",
      "[0.12978986 0.8702101 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9811261  0.01887386]\n",
      "[0.27777967 0.72222036]\n",
      "[0.95431876 0.04568123]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.9489762  0.05102382]\n",
      "[0.07718495 0.922815  ]\n",
      "[0.9141024  0.08589759]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.535077   0.46492308]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9799788  0.02002111]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9819854  0.01801459]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.9256975  0.07430244]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.7344602  0.26553985]\n",
      "[0.02123091 0.97876906]\n",
      "[0.97135735 0.02864261]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.535077   0.46492308]\n",
      "[0.01757339 0.98242664]\n",
      "[0.98186845 0.01813148]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.9813477  0.01865227]\n",
      "[0.01757339 0.98242664]\n",
      "[0.9141024  0.08589759]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.9489762  0.05102382]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.9763949  0.02360513]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9819854  0.01801459]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.96287286 0.03712709]\n",
      "[0.10046926 0.8995307 ]\n",
      "[0.90016395 0.099836  ]\n",
      "[0.03723959 0.96276045]\n",
      "[0.9818294  0.01817061]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.6408268  0.35917315]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.98067516 0.01932482]\n",
      "[0.03393341 0.96606654]\n",
      "[0.6895815  0.31041852]\n",
      "[0.10952325 0.89047676]\n",
      "[0.86460304 0.13539691]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.9489762  0.05102382]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.9815668  0.01843324]\n",
      "[0.08433735 0.91566265]\n",
      "[0.83990735 0.16009264]\n",
      "[0.03393341 0.96606654]\n",
      "[0.97135735 0.02864261]\n",
      "[0.03393341 0.96606654]\n",
      "[0.9763949  0.02360513]\n",
      "[0.10046926 0.8995307 ]\n",
      "[0.535077   0.46492308]\n",
      "[0.01931746 0.9806825 ]\n",
      "[0.9811261  0.01887386]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9804458  0.01955424]\n",
      "[0.27777967 0.72222036]\n",
      "[0.98067516 0.01932482]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.90016395 0.099836  ]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9819854  0.01801459]\n",
      "[0.12978986 0.8702101 ]\n",
      "[0.97878784 0.02121214]\n",
      "[0.03723959 0.96276045]\n",
      "[0.588979   0.41102096]\n",
      "[0.12978986 0.8702101 ]\n",
      "[0.98190755 0.01809244]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9819854  0.01801459]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.588979   0.41102096]\n",
      "[0.1531616  0.84683836]\n",
      "[0.9763949  0.02360513]\n",
      "[0.3238152 0.6761848]\n",
      "[0.9256975  0.07430244]\n",
      "[0.12978986 0.8702101 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.06452356 0.9354764 ]\n",
      "[0.98090196 0.01909803]\n",
      "[0.42607644 0.5739236 ]\n",
      "[0.98206294 0.01793707]\n",
      "[0.07718495 0.922815  ]\n",
      "[0.9817509  0.01824913]\n",
      "[0.01321834 0.98678166]\n",
      "[0.9591261  0.04087394]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9819465  0.01805348]\n",
      "[0.27777967 0.72222036]\n",
      "[0.97878784 0.02121214]\n",
      "[0.01757339 0.98242664]\n",
      "[0.98067516 0.01932482]\n",
      "[0.37353387 0.6264661 ]\n",
      "[0.86460304 0.13539691]\n",
      "[0.27777967 0.72222036]\n",
      "[0.88425016 0.11574983]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.9817902  0.01820983]\n",
      "[0.01321834 0.98678166]\n",
      "[0.93572605 0.06427391]\n",
      "[0.0408543 0.9591457]\n",
      "[0.9816325  0.01836751]\n",
      "[0.0408543 0.9591457]\n",
      "[0.7744026  0.22559732]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.97963667 0.02036336]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.9591261  0.04087394]\n",
      "[0.1531616  0.84683836]\n",
      "[0.9820242  0.01797579]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.98186845 0.01813148]\n",
      "[0.27777967 0.72222036]\n",
      "[0.96890175 0.03109829]\n",
      "[0.01757339 0.98242664]\n",
      "[0.94289505 0.05710501]\n",
      "[0.02123091 0.97876906]\n",
      "[0.9815668  0.01843324]\n",
      "[0.10046926 0.8995307 ]\n",
      "[0.9750354  0.02496454]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9817115 0.0182885]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.9141024  0.08589759]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.48034266 0.5196573 ]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9813477  0.01865227]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.97762257 0.02237739]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.9661509  0.03384911]\n",
      "[0.3238152 0.6761848]\n",
      "[0.9819854  0.01801459]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9802137  0.01978632]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.7744026  0.22559732]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9815668  0.01843324]\n",
      "[0.04911499 0.95088506]\n",
      "[0.9489762  0.05102382]\n",
      "[0.01757339 0.98242664]\n",
      "[0.973278   0.02672207]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.7744026  0.22559732]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9763949  0.02360513]\n",
      "[0.0408543 0.9591457]\n",
      "[0.9817115 0.0182885]\n",
      "[0.23600453 0.7639954 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.86460304 0.13539691]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9141024  0.08589759]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9821401  0.01785989]\n",
      "[0.02123091 0.97876906]\n",
      "[0.93572605 0.06427391]\n",
      "[0.03393341 0.96606654]\n",
      "[0.80937594 0.19062406]\n",
      "[0.01321834 0.98678166]\n",
      "[0.9816325  0.01836751]\n",
      "[0.07718495 0.922815  ]\n",
      "[0.9821401  0.01785989]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.9821786  0.01782142]\n",
      "[0.27777967 0.72222036]\n",
      "[0.9804458  0.01955424]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9763949  0.02360513]\n",
      "[0.23600453 0.7639954 ]\n",
      "[0.6895815  0.31041852]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.535077   0.46492308]\n",
      "[0.01931746 0.9806825 ]\n",
      "[0.9819854  0.01801459]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.7344602  0.26553985]\n",
      "[0.1531616  0.84683836]\n",
      "[0.9813477  0.01865227]\n",
      "[0.01757339 0.98242664]\n",
      "[0.9804458  0.01955424]\n",
      "[0.03393341 0.96606654]\n",
      "[0.9819465  0.01805348]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9802137  0.01978632]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.83990735 0.16009264]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.02815053 0.97184944]\n",
      "[0.97135735 0.02864261]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.6895815  0.31041852]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.9817509  0.01824913]\n",
      "[0.23600453 0.7639954 ]\n",
      "[0.9813477  0.01865227]\n",
      "[0.04480353 0.9551965 ]\n",
      "[0.9817509  0.01824913]\n",
      "[0.23600453 0.7639954 ]\n",
      "[0.98090196 0.01909803]\n",
      "[0.3238152 0.6761848]\n",
      "[0.9817509  0.01824913]\n",
      "[0.02123091 0.97876906]\n",
      "[0.97963667 0.02036336]\n",
      "[0.3238152 0.6761848]\n",
      "[0.97963667 0.02036336]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.97135735 0.02864261]\n",
      "[0.37353387 0.6264661 ]\n",
      "[0.98186845 0.01813148]\n",
      "[0.03393341 0.96606654]\n",
      "[0.9817115 0.0182885]\n",
      "[0.37353387 0.6264661 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.05894332 0.9410566 ]\n",
      "[0.98067516 0.01932482]\n",
      "[0.1531616  0.84683836]\n",
      "[0.6895815  0.31041852]\n",
      "[0.14107172 0.85892826]\n",
      "[0.6408268  0.35917315]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.9817115 0.0182885]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9256975  0.07430244]\n",
      "[0.02815053 0.97184944]\n",
      "[0.97963667 0.02036336]\n",
      "[0.02562979 0.9743702 ]\n",
      "[0.90016395 0.099836  ]\n",
      "[0.02562979 0.9743702 ]\n",
      "[0.98067516 0.01932482]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.90016395 0.099836  ]\n",
      "[0.11928495 0.880715  ]\n",
      "[0.97963667 0.02036336]\n",
      "[0.42607644 0.5739236 ]\n",
      "[0.98206294 0.01793707]\n",
      "[0.02815053 0.97184944]\n",
      "[0.98090196 0.01909803]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9802137  0.01978632]\n",
      "[0.02562979 0.9743702 ]\n",
      "[0.535077   0.46492308]\n",
      "[0.42607644 0.5739236 ]\n",
      "[0.9820242  0.01797579]\n",
      "[0.04480353 0.9551965 ]\n",
      "[0.973278   0.02672207]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.9813477  0.01865227]\n",
      "[0.06452356 0.9354764 ]\n",
      "[0.9661509  0.03384911]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.97878784 0.02121214]\n",
      "[0.05894332 0.9410566 ]\n",
      "[0.588979   0.41102096]\n",
      "[0.01598421 0.9840158 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.9820242  0.01797579]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.9750354  0.02496454]\n",
      "[0.01757339 0.98242664]\n",
      "[0.535077   0.46492308]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.98206294 0.01793707]\n",
      "[0.02815053 0.97184944]\n",
      "[0.96287286 0.03712709]\n",
      "[0.05381792 0.9461821 ]\n",
      "[0.9811261  0.01887386]\n",
      "[0.1531616  0.84683836]\n",
      "[0.9817509  0.01824913]\n",
      "[0.01757339 0.98242664]\n",
      "[0.98206294 0.01793707]\n",
      "[0.3238152 0.6761848]\n",
      "[0.98090196 0.01909803]\n",
      "[0.10046926 0.8995307 ]\n",
      "[0.9819465  0.01805348]\n",
      "[0.02562979 0.9743702 ]\n",
      "[0.9799788  0.02002111]\n",
      "[0.0408543 0.9591457]\n",
      "[0.98090196 0.01909803]\n",
      "[0.07059237 0.9294076 ]\n",
      "[0.9811261  0.01887386]\n",
      "[0.10046926 0.8995307 ]\n",
      "[0.80937594 0.19062406]\n",
      "[0.19895934 0.8010407 ]\n",
      "[0.9821016  0.01789844]\n",
      "[0.10952325 0.89047676]\n",
      "[0.96287286 0.03712709]\n",
      "[0.1531616  0.84683836]\n",
      "[0.981672   0.01832796]\n",
      "[0.03091131 0.9690887 ]\n",
      "[0.7344602  0.26553985]\n",
      "[0.3238152 0.6761848]\n",
      "[0.96890175 0.03109829]\n",
      "[0.17987208 0.8201279 ]\n",
      "[0.95431876 0.04568123]\n",
      "[0.10952325 0.89047676]\n",
      "[0.9818294  0.01817061]\n",
      "[0.0408543 0.9591457]\n",
      "[0.9763949  0.02360513]\n",
      "[0.02562979 0.9743702 ]\n",
      "[0.9817509  0.01824913]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.9816325  0.01836751]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.9816325  0.01836751]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.9256975  0.07430244]\n",
      "[0.02332936 0.9766707 ]\n",
      "[0.7344602  0.26553985]\n",
      "[0.01453661 0.9854634 ]\n",
      "[0.9815668  0.01843324]\n",
      "[0.01757339 0.98242664]\n",
      "[0.98190755 0.01809244]\n",
      "[0.10952325 0.89047676]\n",
      "[0.95431876 0.04568123]\n",
      "[0.08433735 0.91566265]\n",
      "[0.973278   0.02672207]\n",
      "[0.16608731 0.8339127 ]\n",
      "[0.96890175 0.03109829]\n",
      "[0.09208641 0.9079136 ]\n",
      "[0.83990735 0.16009264]\n",
      "[0.01453661 0.9854634 ]\n"
     ]
    }
   ],
   "source": [
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
